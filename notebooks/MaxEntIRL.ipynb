{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqhUr-M7gfS7"
   },
   "source": [
    "# Sample program for maximum entropy inverse reinforcement learning. \n",
    "This notebook implements maximum entropy inverse reinforcement learning. The algorithms are evaluated on the deterministic [FrozenLake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) provided by the [OpenAI gym](https://github.com/openai/gym). There are 8x8 possible states and 4 discrete deterministic actions.\n",
    "* https://github.com/yasufumy/python_irl\n",
    "* https://github.com/yrlu/irl-imitation\n",
    "* https://github.com/harpribot/IRL-maxent\n",
    "\n",
    "References\n",
    "* B. D. Ziebart, A. Maas, J. Andrew Bagnell, and A. K. Dey. (2008). [Maximum Entropy Inverse Reinforcement Learning](https://www.aaai.org/Library/AAAI/2008/aaai08-227.php). In Proc. of AAAI. \n",
    "* M. Wulfmeier, D. Rao, D. ZengWang, P. Ondruska,\n",
    "and I. Posner. (2017). [Large-scale cost function learning for path planning using deep inverse\n",
    "reinforcement learning](https://doi.org/10.1177/0278364917722396). Internationa Journal of Robotics Research, 36(10): 1073-1087.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ANzLep6osEvU"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FrozenLakeEnv.__init__() got an unexpected keyword argument 'new_step_api'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# import random\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# from itertools import product\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# %matplotlib inline\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFrozenLake-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m8x8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_slippery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_step_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\research\\BILT-B\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:654\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[0;32m    649\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed render_mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m although \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt implement human-rendering natively. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym tried to apply the HumanRendering wrapper but it looks like your environment is using the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrendering API, which is not supported by the HumanRendering wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    656\u001b[0m \u001b[38;5;66;03m# Copies the environment creation specification and kwargs to add to the environment specification details\u001b[39;00m\n\u001b[0;32m    657\u001b[0m spec_ \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(spec_)\n",
      "File \u001b[1;32m~\\Documents\\research\\BILT-B\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:642\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 642\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_creator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    645\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[0;32m    647\u001b[0m     ):\n",
      "\u001b[1;31mTypeError\u001b[0m: FrozenLakeEnv.__init__() got an unexpected keyword argument 'new_step_api'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "# import random\n",
    "# from itertools import product\n",
    "# %matplotlib inline\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode='ansi', desc=None, map_name='8x8', is_slippery=False, new_step_api=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAu1AnqwvxRT"
   },
   "source": [
    "# Value iteration\n",
    "Applying the Bellman optimality operator to find the optimal state-action value function. \n",
    "$$V(s) \\leftarrow \\max_a Q(s, a), \\quad Q(s, a) = r(s) + \\gamma \\sum_{s'} p_T (s' \\mid s, a) V(s').$$\n",
    "compute_action_value(state) represents the action value at state \"state.\" The optimal policy is approximated by setting $\\beta$ to a large value.\n",
    "$$\\pi (a \\mid s) = \\frac{\\exp (\\beta Q(s, a))} {\\sum_{a'} \\exp (\\beta Q(s, a'))},$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0JcQH-wsaJO"
   },
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    \n",
    "    def __call__(self, gamma=0.9, beta=500, epslion=1e-5):\n",
    "        n_states = env.observation_space.n\n",
    "        n_actions = env.action_space.n\n",
    "        V = np.random.rand(n_states)\n",
    "\n",
    "        def compute_action_value(state):\n",
    "            qA = np.zeros(n_actions)\n",
    "            for action in range(n_actions):\n",
    "                for prob, next_state, reward, done in self.env.P[state][action]:\n",
    "                    qA[action] += prob * (reward + gamma * V[next_state])\n",
    "            return qA\n",
    "\n",
    "\n",
    "        def compute_softmax_policy(beta):\n",
    "            policy = np.zeros([n_states, n_actions])\n",
    "            for state in range(n_states):\n",
    "                policy[state] = beta*compute_action_value(state)\n",
    "            policy -= policy.max(axis=1, keepdims=True)\n",
    "            policy = np.exp(policy) / np.exp(policy).sum(axis=1, keepdims=True)\n",
    "\n",
    "            return policy\n",
    "\n",
    "        V_error = list([])\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(n_states):\n",
    "                qA = compute_action_value(state)\n",
    "                max_q = qA.max()\n",
    "                delta = max(delta, np.abs(max_q - V[state]))\n",
    "                V[state] = max_q\n",
    "            V_error.append(delta)\n",
    "            if delta < epslion:\n",
    "                break\n",
    "\n",
    "        policy = compute_softmax_policy(beta)\n",
    "\n",
    "        return V, policy, V_error\n",
    "\n",
    "\n",
    "def visualize_V(V, V_error):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    im1 = axs[0].pcolor(V.reshape(8, 8)[::-1, :])\n",
    "    axs[0].set_title('state-value function, V')\n",
    "    axs[0].set_aspect(1.0/axs[0].get_data_ratio(), adjustable='box')\n",
    "    fig.colorbar(im1, ax=axs[0])\n",
    "    axs[1].plot(V_error)\n",
    "    axs[1].set_xlabel('total number of iterations')\n",
    "    axs[1].set_ylabel('error')\n",
    "    axs[1].set_title('convergence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA8NNtyYCHDs"
   },
   "source": [
    "## Calculate the state value and retrieve the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VP8niyz9s-aP"
   },
   "outputs": [],
   "source": [
    "# np.random.seed(seed=0)\n",
    "value_iteration = ValueIteration(env)\n",
    "V, policy, V_error = value_iteration(gamma=0.9, beta=500)\n",
    "visualize_V(V, V_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the optimal behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "total_rewards = 0\n",
    "while not done:\n",
    "    print(env.render(mode='ansi'))\n",
    "    action = np.random.multinomial(1, policy[state]).argmax()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_rewards += reward\n",
    "print(env.render(mode='ansi'))\n",
    "print('total rewards: %f' % (total_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVbz1-bGv9Sy"
   },
   "source": [
    "## Collect the expert data by running the optimal policy\n",
    "The expert policy is given by the optimal policy trained with the original reward function. Then, generate a set of state-action sequences to collect the expert data $\\mathcal{D}^E = \\{ \\tau_k \\}_{k=1}^{N^E}$, $\\tau_k = (s_0^k, s_1^k, \\ldots, s_T^k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TWhpwzvts5t"
   },
   "outputs": [],
   "source": [
    "def sample_trajectories(env, policy, n_steps=10, n_trajectories=100, initial_states=None):\n",
    "  if initial_states is None:\n",
    "    states = np.random.choice(np.arange(0, env.nS), n_trajectories)\n",
    "  else:\n",
    "    states = initial_states\n",
    "\n",
    "  trajectories = []\n",
    "  for state in states:\n",
    "    env.reset()\n",
    "    env.s = state\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    for i in range(n_steps):\n",
    "      action = np.random.multinomial(1, policy[state]).argmax()\n",
    "      trajectory.append(state)\n",
    "      state, reward, done, info = env.step(action)\n",
    "      if done:\n",
    "        trajectory.extend([state] * (n_steps - len(trajectory)))\n",
    "        break\n",
    "\n",
    "      trajectories.append(trajectory)\n",
    "\n",
    "  return np.array(trajectories)\n",
    "\n",
    "trajectories = sample_trajectories(env, policy)\n",
    "print(trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t00FcY96wH14"
   },
   "source": [
    "## Compute the expected feature count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd5uJn8GEotK"
   },
   "outputs": [],
   "source": [
    "def compute_feature(n_features, trajectories):\n",
    "  n_trajectories, n_steps = trajectories.shape\n",
    "  phi = np.zeros(n_features)\n",
    "  traj_flatten = trajectories.ravel()\n",
    "  for state in traj_flatten:\n",
    "      phi[state] += 1\n",
    "  phi /= n_trajectories\n",
    "\n",
    "  return phi\n",
    "\n",
    "experts_feature = compute_feature(env.nS, trajectories)\n",
    "#experts_feature = np.load('experts_feature.npy')\n",
    "print(experts_feature.reshape(4, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmr1XXtKwPWp"
   },
   "source": [
    "# Maximum entropy inverse reinforcement learning\n",
    "The probability to generate the state sequence $\\tau$ is given by\n",
    "$$p(\\tau \\mid w) = \\frac{1}{Z(w)} \\exp \\left( R(\\tau; w) \\right), \\quad Z(w) = \\sum_{\\tau'} \\exp \\left( R(\\tau'; w) \\right), $$\n",
    "where $R(\\tau; w)$ is the total return:\n",
    "$$R(\\tau; w) = \\sum_{t=0}^T \\gamma^t r(s_t; w), $$\n",
    "$w$ is the parameter of the reward function, and $\\gamma$ is the discount factor. For the given expert data, the log likelihood is calculated by\n",
    "$$\\mathcal{L}(w) = \\frac{1}{N^E} \\sum_{k=1}^{N^E} \\ln p(\\tau_k; w).$$\n",
    "Then, its gradient is given by\n",
    "$$\\nabla_w \\mathcal{L}(w) = \\frac{1}{N^E} \\sum_{k=1}^{N^E} \\nabla_w R(\\tau_k; w) - \\sum_{\\tau} p(\\tau \\mid w) \\nabla_w R(\\tau; w).$$\n",
    "The reward parameter is updated by the stochastic gradient ascent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyELh02AFyxU"
   },
   "outputs": [],
   "source": [
    "def compute_visitation(env, policy, trajectories):\n",
    "  n_states = env.nS\n",
    "  n_actions = env.nA\n",
    "  probs = env.P\n",
    "  n_trajectories, n_steps = trajectories.shape\n",
    "\n",
    "  mu = np.zeros((n_steps, n_states))\n",
    "  for trajectory in trajectories:\n",
    "    mu[0, trajectory[0]] += 1\n",
    "  mu /= n_trajectories\n",
    "\n",
    "  states = range(n_states)\n",
    "  actions = range(n_actions)\n",
    "  for t in range(1, n_steps):\n",
    "    for state, action in product(states, actions):\n",
    "      for prob, next_state, _, _ in probs[state][action]:\n",
    "        mu[t][next_state] += mu[t-1][state] * policy[state][action] * prob\n",
    "\n",
    "  return mu.sum(axis=0)\n",
    "\n",
    "class MaxEntIRL:\n",
    "  def __init__(self, env, experts_feature, trajectories):\n",
    "    self.env = env\n",
    "    self.experts_feature = experts_feature\n",
    "    self.trajectories = trajectories\n",
    "\n",
    "  def __call__(self, policy, n_epochs, gamma=0.9, beta=500, epsilon=1e-5):\n",
    "    value_iteration = ValueIteration(self.env)\n",
    "    reward_function = np.zeros(self.env.nS)\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "      V, policy = value_iteration(gamma, beta, epsilon, reward_function=reward_function)\n",
    "      learners_feature = compute_visitation(self.env, policy, self.trajectories)\n",
    "      grad = self.experts_feature - learners_feature\n",
    "      reward_function += learning_rate * grad\n",
    "\n",
    "    return reward_function, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpKYvp3joZKp"
   },
   "outputs": [],
   "source": [
    "maxent_irl = MaxEntIRL(env, experts_feature, trajectories)\n",
    "reward_function, V = maxent_irl(policy, n_epochs=50, gamma=0.9)\n",
    "print(reward_function.reshape(4, 4)[::1, :])\n",
    "print(V.reshape(4, 4)[::1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQdJSIjX2oLq"
   },
   "source": [
    "## Show results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcKRxbNWw0mV"
   },
   "outputs": [],
   "source": [
    "def plot_results(reward_function, V):\n",
    "  fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "  im1 = axs[0].pcolor(reward_function.reshape(4, 4)[::-1, :])\n",
    "  axs[0].set_title('reward function')\n",
    "  fig.colorbar(im1, ax=axs[0])\n",
    "  im2 = axs[1].pcolor(V.reshape(4, 4)[::-1, :])\n",
    "  axs[1].set_title('value function')\n",
    "  fig.colorbar(im2, ax=axs[1])\n",
    "  plt.show()\n",
    "\n",
    "plot_results(reward_function, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoQ4NpmHeoRd"
   },
   "source": [
    "# Modified maximum inverse reinforcement learning\n",
    "\n",
    "The second term of the gradient of the log likelihood is approximated using samples generated by the learner's policy:\n",
    "$$\\nabla_w \\mathcal{L}(w) = \\frac{1}{N^E} \\sum_{k=1}^{N^E} \\nabla_w R(\\tau_k; w) - \\frac{1}{N^L} \\sum_{j=1}^{N^L} \\nabla_w R(\\tau_j; w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSEYQUKxYFRG"
   },
   "outputs": [],
   "source": [
    "class MaxEntIRLSample:\n",
    "  def __init__(self, env, experts_feature, trajectories):\n",
    "    self.env = env\n",
    "    self.experts_feature = experts_feature\n",
    "    self.n_trajectories, self.n_steps = trajectories.shape\n",
    "    self.initial_states = trajectories[:, 0]\n",
    "\n",
    "  def __call__(self, policy, n_epochs, gamma=0.9, beta=500, epsilon=1e-5):\n",
    "    value_iteration = ValueIteration(self.env)\n",
    "    reward_function = np.zeros(self.env.nS)\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "      V, policy = value_iteration(gamma, beta, epsilon, reward_function)\n",
    "      trajectories = sample_trajectories(self.env, policy, self.n_steps,\n",
    "                                         self.n_trajectories,\n",
    "                                         self.initial_states)\n",
    "      learners_feature = compute_feature(self.env.nS, trajectories)\n",
    "      grad = self.experts_feature - learners_feature\n",
    "      reward_function += learning_rate * grad\n",
    "\n",
    "    return reward_function, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPb4jrgdYSG4"
   },
   "outputs": [],
   "source": [
    "maxent_irl_sample = MaxEntIRLSample(env, experts_feature, trajectories)\n",
    "reward_function, V = maxent_irl_sample(policy, n_epochs=50, gamma=0.9)\n",
    "# print(reward_function.reshape(4, 4)[::1, :])\n",
    "# print(V.reshape(4, 4)[::1, :])\n",
    "plot_results(reward_function, V)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MaxEntIRL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
