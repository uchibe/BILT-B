{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqhUr-M7gfS7"
      },
      "source": [
        "# Sample program for maximum entropy inverse reinforcement learning. \n",
        "This notebook implements maximum entropy inverse reinforcement learning (Ziebart et al., 2008; Wulfmeier et al., 2017). The algorithms are evaluated on the deterministic [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) provided by the [Gymnasium](https://gymnasium.farama.org/). There are 8x8 possible states and 4 discrete deterministic actions.The codes of this notebook are based on the following pages:\n",
        "* https://github.com/yasufumy/python_irl\n",
        "* https://github.com/yrlu/irl-imitation\n",
        "* https://github.com/harpribot/IRL-maxent\n",
        "\n",
        "References\n",
        "* B. D. Ziebart, A. Maas, J. Andrew Bagnell, and A. K. Dey. (2008). [Maximum Entropy Inverse Reinforcement Learning](https://www.aaai.org/Library/AAAI/2008/aaai08-227.php). In Proc. of AAAI. \n",
        "* M. Wulfmeier, D. Rao, D. ZengWang, P. Ondruska,\n",
        "and I. Posner. (2017). [Large-scale cost function learning for path planning using deep inverse\n",
        "reinforcement learning](https://doi.org/10.1177/0278364917722396). Internationa Journal of Robotics Research, 36(10): 1073-1087.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANzLep6osEvU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install gymnasium\n",
        "import gymnasium as gym\n",
        "import random\n",
        "from itertools import product\n",
        "# %matplotlib inline\n",
        "\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi', desc=None, map_name='8x8', is_slippery=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAu1AnqwvxRT"
      },
      "source": [
        "# Value iteration\n",
        "Applying the Bellman optimality operator to find the optimal state-action value function. \n",
        "$$V(s) \\leftarrow \\max_a Q(s, a), \\quad Q(s, a) = r(s) + \\gamma \\sum_{s'} p_T (s' \\mid s, a) V(s').$$\n",
        "compute_action_value(state) represents the action value at state \"state.\" For simplicity, the optimal policy is approximated by setting $\\beta$ to a large value.\n",
        "$$\\pi (a \\mid s) = \\frac{\\exp (\\beta Q(s, a))} {\\sum_{a'} \\exp (\\beta Q(s, a'))}.$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0JcQH-wsaJO"
      },
      "outputs": [],
      "source": [
        "class ValueIteration:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    \n",
        "    def __call__(self, gamma=0.95, beta=500, reward_function=None):\n",
        "        n_states = env.observation_space.n\n",
        "        n_actions = env.action_space.n\n",
        "        V = np.random.rand(n_states)\n",
        "        epslion = 1e-6\n",
        "\n",
        "        def compute_action_value(state):\n",
        "            qA = np.zeros(n_actions)\n",
        "            for action in range(n_actions):\n",
        "                for prob, next_state, reward, done in self.env.P[state][action]:\n",
        "                    if reward_function is not None:\n",
        "                        reward = reward_function[state]\n",
        "                    qA[action] += prob * (reward + gamma * V[next_state])\n",
        "            return qA\n",
        "\n",
        "\n",
        "        def compute_softmax_policy(beta):\n",
        "            policy = np.zeros([n_states, n_actions])\n",
        "            for state in range(n_states):\n",
        "                policy[state] = beta*compute_action_value(state)\n",
        "            policy -= policy.max(axis=1, keepdims=True)\n",
        "            policy = np.exp(policy) / np.exp(policy).sum(axis=1, keepdims=True)\n",
        "\n",
        "            return policy\n",
        "\n",
        "        V_error = list([])\n",
        "\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in range(n_states):\n",
        "                qA = compute_action_value(state)\n",
        "                max_q = qA.max()\n",
        "                delta = max(delta, np.abs(max_q - V[state]))\n",
        "                V[state] = max_q\n",
        "            V_error.append(delta)\n",
        "            if delta < epslion:\n",
        "                break\n",
        "\n",
        "        policy = compute_softmax_policy(beta)\n",
        "\n",
        "        return V, policy, V_error\n",
        "\n",
        "\n",
        "def visualize_V(V, V_error):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    im1 = axs[0].pcolor(V.reshape(8, 8)[::-1, :])\n",
        "    axs[0].set_title('state-value function, V')\n",
        "    axs[0].set_aspect(1.0/axs[0].get_data_ratio(), adjustable='box')\n",
        "    fig.colorbar(im1, ax=axs[0])\n",
        "    axs[1].plot(V_error)\n",
        "    axs[1].set_xlabel('total number of iterations')\n",
        "    axs[1].set_ylabel('error')\n",
        "    axs[1].set_title('convergence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA8NNtyYCHDs"
      },
      "source": [
        "## Calculate the state value and retrieve the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP8niyz9s-aP"
      },
      "outputs": [],
      "source": [
        "# np.random.seed(seed=0)\n",
        "value_iteration = ValueIteration(env)\n",
        "V, policy, V_error = value_iteration(gamma=0.9, beta=500)\n",
        "visualize_V(V, V_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QTpDV6G2b--"
      },
      "source": [
        "## Show the optimal behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPtyINkQ2b-_"
      },
      "outputs": [],
      "source": [
        "state, info = env.reset()\n",
        "done = False\n",
        "total_rewards = 0\n",
        "total_steps = 0\n",
        "while not done:\n",
        "    print(env.render())\n",
        "    action = np.random.multinomial(1, policy[state]).argmax()\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    total_rewards += reward\n",
        "    total_steps = total_steps + 1\n",
        "print(env.render())\n",
        "print('total rewards: %f, total_steps: %d' % (total_rewards, total_steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVbz1-bGv9Sy"
      },
      "source": [
        "## Collect the expert data by running the optimal policy\n",
        "The expert policy is given by the optimal policy trained with the original reward function. Then, generate a set of state-action sequences to collect the expert data $\\mathcal{D}^E = \\{ \\tau_k \\}_{k=1}^{N^E}$, $\\tau_k = (s_0^k, s_1^k, \\ldots, s_T^k)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TWhpwzvts5t"
      },
      "outputs": [],
      "source": [
        "def sample_trajectories(env, policy, n_steps=20, n_trajectories=100, initial_states=None):\n",
        "  if initial_states is None:\n",
        "    states = np.random.choice(np.arange(0, env.observation_space.n), n_trajectories)\n",
        "  else:\n",
        "    states = initial_states\n",
        "\n",
        "  trajectories = []\n",
        "  for state in states:\n",
        "    env.reset()\n",
        "    env.s = state\n",
        "    done = False\n",
        "    trajectory = []\n",
        "    for i in range(n_steps):\n",
        "      action = np.random.multinomial(1, policy[state]).argmax()\n",
        "      trajectory.append(state)\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = truncated or terminated \n",
        "      if done:\n",
        "        trajectory.extend([state] * (n_steps - len(trajectory)))\n",
        "        break\n",
        "\n",
        "      trajectories.append(trajectory)\n",
        "\n",
        "  return np.array(trajectories)\n",
        "\n",
        "trajectories = sample_trajectories(env, policy)\n",
        "print(trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t00FcY96wH14"
      },
      "source": [
        "## Compute the expected feature count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd5uJn8GEotK"
      },
      "outputs": [],
      "source": [
        "def compute_feature(n_features, trajectories, gamma=1.0):\n",
        "  n_trajectories, n_steps = trajectories.shape\n",
        "  phi = np.zeros(n_features)\n",
        "  traj_flatten = trajectories.ravel()\n",
        "  for state in traj_flatten:\n",
        "      phi[state] = 1 + gamma*phi[state]\n",
        "  phi /= n_trajectories\n",
        "\n",
        "  return phi\n",
        "\n",
        "experts_feature = compute_feature(env.observation_space.n, trajectories)\n",
        "#experts_feature = np.load('experts_feature.npy')\n",
        "print(experts_feature.reshape(4, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmr1XXtKwPWp"
      },
      "source": [
        "# Maximum entropy inverse reinforcement learning\n",
        "The probability to generate the state sequence $\\tau$ is given by\n",
        "$$p(\\tau \\mid w) = \\frac{1}{Z(w)} \\exp \\left( R(\\tau; w) \\right), \\quad Z(w) = \\sum_{\\tau'} \\exp \\left( R(\\tau'; w) \\right), $$\n",
        "where $R(\\tau; w)$ is the total return:\n",
        "$$R(\\tau; w) = \\sum_{t=0}^T \\gamma^t r(s_t; w), $$\n",
        "$w$ is the parameter of the reward function, and $\\gamma$ is the discount factor. For the given expert data, the log likelihood is calculated by\n",
        "$$\\mathcal{L}(w) = \\frac{1}{N^E} \\sum_{k=1}^{N^E} \\ln p(\\tau_k; w).$$\n",
        "Then, its gradient is given by\n",
        "$$\\nabla_w \\mathcal{L}(w) = \\frac{1}{N^E} \\sum_{k=1}^{N^E} \\nabla_w R(\\tau_k; w) - \\sum_{\\tau} p(\\tau \\mid w) \\nabla_w R(\\tau; w).$$\n",
        "The reward parameter is updated by the stochastic gradient ascent. However, it is not trivial to evaluate the second term of the gradient. Therefore, it is approximated using samples generated by the learner's policy:\n",
        "$$\\nabla_w \\mathcal{L}(w) = \\frac{1}{N^E} \\sum_{k=1}^{N^E} \\nabla_w R(\\tau_k; w) - \\frac{1}{N^L} \\sum_{j=1}^{N^L} \\nabla_w R(\\tau_j; w).$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSEYQUKxYFRG"
      },
      "outputs": [],
      "source": [
        "class MaxEntIRLSample:\n",
        "  def __init__(self, env, experts_feature, trajectories):\n",
        "    self.env = env\n",
        "    self.experts_feature = experts_feature\n",
        "    self.n_trajectories, self.n_steps = trajectories.shape\n",
        "    self.initial_states = trajectories[:, 0]\n",
        "\n",
        "  def __call__(self, policy, n_epochs, gamma=0.9, beta=500, epsilon=1e-5):\n",
        "    value_iteration = ValueIteration(self.env)\n",
        "    reward_function = np.zeros(self.env.observation_space.n)\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "      V, policy, V_error = value_iteration(gamma, beta, reward_function)\n",
        "      trajectories = sample_trajectories(self.env, policy, self.n_steps,\n",
        "                                         self.n_trajectories,\n",
        "                                         self.initial_states)\n",
        "      learners_feature = compute_feature(self.env.observation_space.n, trajectories)\n",
        "      grad = self.experts_feature - learners_feature\n",
        "      reward_function += learning_rate * grad\n",
        "\n",
        "    return reward_function, V\n",
        "\n",
        "def plot_results(reward_function, V):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    im1 = axs[0].pcolor(reward_function.reshape(8, 8)[::-1, :])\n",
        "    axs[0].set_title('reward function')\n",
        "    fig.colorbar(im1, ax=axs[0])\n",
        "    im2 = axs[1].pcolor(V.reshape(8, 8)[::-1, :])\n",
        "    axs[1].set_title('value function')\n",
        "    fig.colorbar(im2, ax=axs[1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPb4jrgdYSG4"
      },
      "outputs": [],
      "source": [
        "maxent_irl_sample = MaxEntIRLSample(env, experts_feature, trajectories)\n",
        "reward_function, V = maxent_irl_sample(policy, n_epochs=50, gamma=0.9)\n",
        "plot_results(reward_function, V)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "MaxEntIRL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}